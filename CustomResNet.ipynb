{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchsummary import summary\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "Eezp523l2AxP"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the ResNet class as a subclass of nn.Module\n",
        "class ResNet(nn.Module):\n",
        "  # Constructor function\n",
        "    def __init__(self, num_classes=10):\n",
        "        # Call the constructor of the parent class\n",
        "        super(ResNet, self).__init__()\n",
        "        # Set the number of input planes to 32\n",
        "        self.in_planes = 32\n",
        "        # Set the kernel size for convolution to (3,1)\n",
        "        self.kernel = (3,1)\n",
        "        # Set the kernel size for skip connections to (1,0)\n",
        "        self.skip_kernel = (1,0)\n",
        "        # Set the number of layers to 4\n",
        "        self.num_layers = 4\n",
        "        # Set the number of blocks in each layer\n",
        "        self.num_blocks = [1, 1, 1, 4]\n",
        "        # Define the first convolution layer\n",
        "        self.conv1 = nn.Conv2d(3, self.in_planes, kernel_size=self.kernel[0],stride=1, padding=self.kernel[1], bias=True)\n",
        "        # Define the batch normalization layer\n",
        "        self.bn1 = nn.BatchNorm2d(self.in_planes)\n",
        "        # Define the layers with specified number of planes, blocks and stride\n",
        "        self.layer1 = self._make_layer(self.in_planes, self.num_blocks[0], stride=1, bias=True)\n",
        "        self.layer1 = self._make_layer(64, 3, stride=1)\n",
        "        self.layer2 = self._make_layer(128, 3, stride=2)\n",
        "        self.layer3 = self._make_layer(256, 3, stride=2) \n",
        "        self.layer4 = self._make_layer(512, 2, stride=2) \n",
        "        # Get the number of features from the last layer\n",
        "        finalshape = list(getattr(self, \"layer\"+str(self.num_layers))[-1].modules())[-2].num_features\n",
        "        # Set the multiplier based on the number of layers\n",
        "        self.multiplier = 4 if self.num_layers == 2 else (2 if self.num_layers == 3 else 1)\n",
        "        # Define the final linear layer with specified number of output classes\n",
        "        self.linear = nn.Linear(finalshape, num_classes)\n",
        "\n",
        "    # Helper function to define a layer\n",
        "    def _make_layer(self, planes, num_blocks, stride, bias=True):\n",
        "      # Create a list of strides for each block in the layer\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        # Create an empty list to store the custom layers\n",
        "        custom_layers = []\n",
        "        # Loop through the strides and define each block in the layer\n",
        "        for stride in strides:\n",
        "            custom_layers.append(nn.Sequential(\n",
        "            nn.Conv2d(self.in_planes, planes, kernel_size=self.kernel[0], stride=stride, padding=self.kernel[1], bias=bias),\n",
        "            nn.BatchNorm2d(planes),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(planes, planes, kernel_size=self.kernel[0], stride=1, padding=self.kernel[1], bias=bias),\n",
        "            nn.BatchNorm2d(planes),\n",
        "            nn.ReLU(inplace=True)\n",
        "        ))\n",
        "            # Update the number of input planes for the next block\n",
        "            self.in_planes = planes\n",
        "            # Return the custom layer as a sequential module\n",
        "        return nn.Sequential(*custom_layers)\n",
        "\n",
        "    # Forward pass function\n",
        "    def forward(self, x):\n",
        "        # Apply the first convolution layer, batch normalization layer and ReLU activation function\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        # Loop through all the layers and apply each layer in sequence\n",
        "        for i in range(1, self.num_layers+1):\n",
        "            out = eval(\"self.layer\" + str(i) + \"(out)\")\n",
        "        # perform global average pooling and flatten output\n",
        "        out = F.avg_pool2d(out, 4*self.multiplier)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        # pass output through fully connected layer and return\n",
        "        out = self.linear(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "uGfyWBRG5LWg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the data augmentation and normalization transforms for training data\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4), # Randomly crop the input image to size 32 with padding of 4 pixels\n",
        "    transforms.RandomHorizontalFlip(), # Randomly flip the input image horizontally\n",
        "    transforms.ToTensor(), # Convert the input image to a PyTorch tensor\n",
        "    # Normalize the input image using the given mean and standard deviation values for each color channel\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "# Define the data normalization transforms for testing data\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(), # Convert the input image to a PyTorch tensor\n",
        "    # Normalize the input image using the given mean and standard deviation values for each color channel\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n"
      ],
      "metadata": {
        "id": "hfiKktWH2GrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CIFAR10 dataset for training\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "# Load the CIFAR10 dataset for testing\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8dgxTB52JE2",
        "outputId": "a36b9919-be54-415d-d175-2e2d6490a188"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the validation set from the training set\n",
        "trainset_size = len(trainset) # get the length of the trainset\n",
        "valset_size = int(trainset_size * 0.2) # calculate the size of validation set as 20% of the trainset\n",
        "trainset_size -= valset_size # subtract the size of validation set from the trainset size\n",
        "trainset, valset = torch.utils.data.random_split(trainset, [trainset_size, valset_size]) # split the trainset into trainset and validation set\n",
        "\n",
        "# create a dataloader for the trainset with batch size of 128 and 2 worker threads for loading data in parallel\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "# create a dataloader for the validation set with batch size of 128 and no shuffling of data\n",
        "valloader = torch.utils.data.DataLoader(valset, batch_size=128, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "SxpZ3XMK_3Ic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "# Define the ResNet model and optimizer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Instantiate the ResNet model and move it to the selected device\n",
        "net = ResNet().to(device)\n",
        "# Print the summary of the model architecture\n",
        "summary(net, input_size=(3, 32, 32))\n",
        "# Define the loss function to be used\n",
        "lossFunction = torch.nn.CrossEntropyLoss(reduction='sum')\n",
        "# Define the learning rate for the optimizer\n",
        "learningRate = 0.1\n",
        "# Define the weight decay for the optimizer\n",
        "weightDecay = 0.0001\n",
        "# Instantiate the optimizer with the specified parameters and the model parameters\n",
        "optimizer = torch.optim.Adadelta(net.parameters(), lr=learningRate, weight_decay=weightDecay)\n",
        "#optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\n",
        "#optimizer1 = optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\n",
        "#optimizer2 = optim.Adam(net.parameters(), lr=0.001, betas=(0.9, 0.999), weight_decay=1e-4)\n",
        "#summary(net, input_size=(3, 32, 32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Srq_fNTy2gp2",
        "outputId": "92b6e5c2-c9f4-4ef4-b594-a4982a530d0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "            Conv2d-3           [-1, 32, 32, 32]           9,248\n",
            "       BatchNorm2d-4           [-1, 32, 32, 32]              64\n",
            "              ReLU-5           [-1, 32, 32, 32]               0\n",
            "            Conv2d-6           [-1, 32, 32, 32]           9,248\n",
            "       BatchNorm2d-7           [-1, 32, 32, 32]              64\n",
            "              ReLU-8           [-1, 32, 32, 32]               0\n",
            "            Conv2d-9           [-1, 64, 16, 16]          18,496\n",
            "      BatchNorm2d-10           [-1, 64, 16, 16]             128\n",
            "             ReLU-11           [-1, 64, 16, 16]               0\n",
            "           Conv2d-12           [-1, 64, 16, 16]          36,928\n",
            "      BatchNorm2d-13           [-1, 64, 16, 16]             128\n",
            "             ReLU-14           [-1, 64, 16, 16]               0\n",
            "           Conv2d-15            [-1, 128, 8, 8]          73,856\n",
            "      BatchNorm2d-16            [-1, 128, 8, 8]             256\n",
            "             ReLU-17            [-1, 128, 8, 8]               0\n",
            "           Conv2d-18            [-1, 128, 8, 8]         147,584\n",
            "      BatchNorm2d-19            [-1, 128, 8, 8]             256\n",
            "             ReLU-20            [-1, 128, 8, 8]               0\n",
            "           Conv2d-21            [-1, 256, 4, 4]         295,168\n",
            "      BatchNorm2d-22            [-1, 256, 4, 4]             512\n",
            "             ReLU-23            [-1, 256, 4, 4]               0\n",
            "           Conv2d-24            [-1, 256, 4, 4]         590,080\n",
            "      BatchNorm2d-25            [-1, 256, 4, 4]             512\n",
            "             ReLU-26            [-1, 256, 4, 4]               0\n",
            "           Conv2d-27            [-1, 256, 4, 4]         590,080\n",
            "      BatchNorm2d-28            [-1, 256, 4, 4]             512\n",
            "             ReLU-29            [-1, 256, 4, 4]               0\n",
            "           Conv2d-30            [-1, 256, 4, 4]         590,080\n",
            "      BatchNorm2d-31            [-1, 256, 4, 4]             512\n",
            "             ReLU-32            [-1, 256, 4, 4]               0\n",
            "           Conv2d-33            [-1, 256, 4, 4]         590,080\n",
            "      BatchNorm2d-34            [-1, 256, 4, 4]             512\n",
            "             ReLU-35            [-1, 256, 4, 4]               0\n",
            "           Conv2d-36            [-1, 256, 4, 4]         590,080\n",
            "      BatchNorm2d-37            [-1, 256, 4, 4]             512\n",
            "             ReLU-38            [-1, 256, 4, 4]               0\n",
            "           Conv2d-39            [-1, 256, 4, 4]         590,080\n",
            "      BatchNorm2d-40            [-1, 256, 4, 4]             512\n",
            "             ReLU-41            [-1, 256, 4, 4]               0\n",
            "           Conv2d-42            [-1, 256, 4, 4]         590,080\n",
            "      BatchNorm2d-43            [-1, 256, 4, 4]             512\n",
            "             ReLU-44            [-1, 256, 4, 4]               0\n",
            "           Linear-45                   [-1, 10]           2,570\n",
            "================================================================\n",
            "Total params: 4,729,610\n",
            "Trainable params: 4,729,610\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 3.88\n",
            "Params size (MB): 18.04\n",
            "Estimated Total Size (MB): 21.93\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0ic3Juf2bb7",
        "outputId": "9163640b-e189-4462-bee9-5a241458021c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIBfNspg2e8C",
        "outputId": "90d5ef4a-265d-4b45-97b4-e9e82dfaffd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Adadelta (\n",
              "Parameter Group 0\n",
              "    differentiable: False\n",
              "    eps: 1e-06\n",
              "    foreach: None\n",
              "    lr: 0.1\n",
              "    maximize: False\n",
              "    rho: 0.9\n",
              "    weight_decay: 0.0001\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define the optimizer and loss function\n",
        "#optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\n",
        "lossFunction = torch.nn.CrossEntropyLoss(reduction='sum')\n",
        "# Set the learning rate and weight decay for the optimizer\n",
        "learningRate = 0.1\n",
        "weightDecay = 0.0001\n",
        "# Define the optimizer to be used\n",
        "optimizer = torch.optim.Adadelta(net.parameters(), lr=learningRate, weight_decay=weightDecay)\n",
        "# Define the criterion to be used for calculating the loss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "# Set the number of epochs to train the model\n",
        "num_epochs = 300\n",
        "# Initialize empty lists to store the training and validation loss and accuracy values\n",
        "train_loss, train_acc = [], []\n",
        "val_loss, val_acc = [], []\n",
        "# Initialize a variable to keep track of the best validation accuracy so far\n",
        "best_val_acc = 0.0\n",
        "\n",
        "# Loop through each epoch\n",
        "for epoch in range(num_epochs):\n",
        "  # Set the model to training mode\n",
        "    net.train()\n",
        "    # Initialize variables to keep track of the running loss and accuracy for the current epoch\n",
        "    running_loss, running_acc = 0.0, 0.0\n",
        "    # Loop through each batch in the training data\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "      # Get the input and label data for the current batch, and move it to the GPU if available\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "        # Zero out the gradients from the previous iteration\n",
        "        optimizer.zero_grad()\n",
        "        # Pass the input data through the model to get the output predictions\n",
        "        outputs = net(inputs)\n",
        "        # Calculate the loss between the predictions and the true labels\n",
        "        loss = criterion(outputs, labels)\n",
        "        # Backpropagate the loss through the model to calculate gradients for each parameter\n",
        "        loss.backward()\n",
        "        # Update the model parameters using the optimizer and the calculated gradients\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Add the current batch's loss and accuracy to the running totals for the epoch\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        running_acc += (predicted == labels).sum().item()\n",
        "\n",
        "    # Calculate the average loss and accuracy for the epoch, and add them to the respective lists\n",
        "    train_loss.append(running_loss / len(trainloader))\n",
        "    train_acc.append(running_acc / len(trainloader.dataset))\n",
        "    \n",
        "    # Set the model to evaluation mode\n",
        "    net.eval()\n",
        "    # Initialize variables to keep track of the running loss and accuracy for the current epoch\n",
        "    running_loss, running_acc = 0.0, 0.0\n",
        "    # Loop through each batch in the validation data\n",
        "    with torch.no_grad():\n",
        "      # loop over the validation data\n",
        "        for i, data in enumerate(valloader, 0):\n",
        "          # Get the input and label data for the current batch, and move it to the GPU if available\n",
        "            inputs, labels = data[0].to(device), data[1].to(device) # move the input and labels to the device (CPU or GPU)\n",
        "            outputs = net(inputs) # forward pass\n",
        "            loss = criterion(outputs, labels) # compute the loss\n",
        "            \n",
        "            running_loss += loss.item() # update the running loss\n",
        "            _, predicted = torch.max(outputs.data, 1) # get the predicted class labels\n",
        "            running_acc += (predicted == labels).sum().item() # update the running accuracy\n",
        "\n",
        "    val_loss.append(running_loss / len(valloader)) # append the average validation loss for this epoch\n",
        "    val_acc.append(running_acc / len(valloader.dataset)) # append the average validation accuracy for this epoch\n",
        "    \n",
        "    # Print the training and validation accuracy for every epoch\n",
        "    print('Epoch [{}/{}], Train Loss: {:.4f}, Train Acc: {:.4f}, Val Loss: {:.4f}, Val Acc: {:.4f}'.format(\n",
        "        epoch+1, num_epochs, train_loss[-1], train_acc[-1], val_loss[-1], val_acc[-1]))\n",
        "    \n",
        "    # Save the model with the best validation accuracy\n",
        "    if val_acc[-1] > best_val_acc:\n",
        "        best_val_acc = val_acc[-1]\n",
        "        torch.save(net.state_dict(), 'best_model.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGKr_qNA4Bzf",
        "outputId": "4d85c0f4-23c0-4f9f-fb07-6d7d0efe505b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/300], Train Loss: 1.6011, Train Acc: 0.4067, Val Loss: 1.5327, Val Acc: 0.4602\n",
            "Epoch [2/300], Train Loss: 1.1933, Train Acc: 0.5692, Val Loss: 1.2747, Val Acc: 0.5442\n",
            "Epoch [3/300], Train Loss: 0.9688, Train Acc: 0.6575, Val Loss: 1.0661, Val Acc: 0.6238\n",
            "Epoch [4/300], Train Loss: 0.8316, Train Acc: 0.7085, Val Loss: 0.9306, Val Acc: 0.6834\n",
            "Epoch [5/300], Train Loss: 0.7355, Train Acc: 0.7430, Val Loss: 0.8324, Val Acc: 0.7151\n",
            "Epoch [6/300], Train Loss: 0.6685, Train Acc: 0.7684, Val Loss: 0.8144, Val Acc: 0.7188\n",
            "Epoch [7/300], Train Loss: 0.6180, Train Acc: 0.7873, Val Loss: 0.7811, Val Acc: 0.7257\n",
            "Epoch [8/300], Train Loss: 0.5717, Train Acc: 0.8042, Val Loss: 0.7602, Val Acc: 0.7485\n",
            "Epoch [9/300], Train Loss: 0.5295, Train Acc: 0.8169, Val Loss: 0.7099, Val Acc: 0.7649\n",
            "Epoch [10/300], Train Loss: 0.4977, Train Acc: 0.8276, Val Loss: 0.6663, Val Acc: 0.7738\n",
            "Epoch [11/300], Train Loss: 0.4696, Train Acc: 0.8365, Val Loss: 0.6529, Val Acc: 0.7750\n",
            "Epoch [12/300], Train Loss: 0.4493, Train Acc: 0.8445, Val Loss: 0.6156, Val Acc: 0.7916\n",
            "Epoch [13/300], Train Loss: 0.4201, Train Acc: 0.8541, Val Loss: 0.5124, Val Acc: 0.8267\n",
            "Epoch [14/300], Train Loss: 0.4041, Train Acc: 0.8598, Val Loss: 0.5772, Val Acc: 0.8081\n",
            "Epoch [15/300], Train Loss: 0.3811, Train Acc: 0.8674, Val Loss: 0.6216, Val Acc: 0.8006\n",
            "Epoch [16/300], Train Loss: 0.3642, Train Acc: 0.8740, Val Loss: 0.6104, Val Acc: 0.8031\n",
            "Epoch [17/300], Train Loss: 0.3487, Train Acc: 0.8797, Val Loss: 0.5454, Val Acc: 0.8190\n",
            "Epoch [18/300], Train Loss: 0.3323, Train Acc: 0.8855, Val Loss: 0.5290, Val Acc: 0.8244\n",
            "Epoch [19/300], Train Loss: 0.3198, Train Acc: 0.8896, Val Loss: 0.6247, Val Acc: 0.7980\n",
            "Epoch [20/300], Train Loss: 0.3047, Train Acc: 0.8944, Val Loss: 0.4760, Val Acc: 0.8406\n",
            "Epoch [21/300], Train Loss: 0.2929, Train Acc: 0.8978, Val Loss: 0.5535, Val Acc: 0.8204\n",
            "Epoch [22/300], Train Loss: 0.2821, Train Acc: 0.9019, Val Loss: 0.4828, Val Acc: 0.8420\n",
            "Epoch [23/300], Train Loss: 0.2731, Train Acc: 0.9058, Val Loss: 0.4946, Val Acc: 0.8404\n",
            "Epoch [24/300], Train Loss: 0.2590, Train Acc: 0.9098, Val Loss: 0.5359, Val Acc: 0.8356\n",
            "Epoch [25/300], Train Loss: 0.2514, Train Acc: 0.9120, Val Loss: 0.4818, Val Acc: 0.8443\n",
            "Epoch [26/300], Train Loss: 0.2433, Train Acc: 0.9143, Val Loss: 0.4973, Val Acc: 0.8382\n",
            "Epoch [27/300], Train Loss: 0.2351, Train Acc: 0.9186, Val Loss: 0.5307, Val Acc: 0.8353\n",
            "Epoch [28/300], Train Loss: 0.2263, Train Acc: 0.9206, Val Loss: 0.4668, Val Acc: 0.8555\n",
            "Epoch [29/300], Train Loss: 0.2161, Train Acc: 0.9238, Val Loss: 0.5268, Val Acc: 0.8379\n",
            "Epoch [30/300], Train Loss: 0.2094, Train Acc: 0.9275, Val Loss: 0.4575, Val Acc: 0.8566\n",
            "Epoch [31/300], Train Loss: 0.1970, Train Acc: 0.9301, Val Loss: 0.5469, Val Acc: 0.8418\n",
            "Epoch [32/300], Train Loss: 0.1926, Train Acc: 0.9328, Val Loss: 0.4699, Val Acc: 0.8580\n",
            "Epoch [33/300], Train Loss: 0.1871, Train Acc: 0.9336, Val Loss: 0.4629, Val Acc: 0.8593\n",
            "Epoch [34/300], Train Loss: 0.1798, Train Acc: 0.9366, Val Loss: 0.4825, Val Acc: 0.8547\n",
            "Epoch [35/300], Train Loss: 0.1784, Train Acc: 0.9363, Val Loss: 0.4593, Val Acc: 0.8575\n",
            "Epoch [36/300], Train Loss: 0.1671, Train Acc: 0.9407, Val Loss: 0.5018, Val Acc: 0.8554\n",
            "Epoch [37/300], Train Loss: 0.1646, Train Acc: 0.9413, Val Loss: 0.4786, Val Acc: 0.8542\n",
            "Epoch [38/300], Train Loss: 0.1596, Train Acc: 0.9438, Val Loss: 0.4770, Val Acc: 0.8573\n",
            "Epoch [39/300], Train Loss: 0.1527, Train Acc: 0.9458, Val Loss: 0.5715, Val Acc: 0.8399\n",
            "Epoch [40/300], Train Loss: 0.1545, Train Acc: 0.9450, Val Loss: 0.4569, Val Acc: 0.8645\n",
            "Epoch [41/300], Train Loss: 0.1426, Train Acc: 0.9490, Val Loss: 0.4835, Val Acc: 0.8611\n",
            "Epoch [42/300], Train Loss: 0.1392, Train Acc: 0.9505, Val Loss: 0.4702, Val Acc: 0.8665\n",
            "Epoch [43/300], Train Loss: 0.1341, Train Acc: 0.9534, Val Loss: 0.4727, Val Acc: 0.8628\n",
            "Epoch [44/300], Train Loss: 0.1336, Train Acc: 0.9523, Val Loss: 0.4503, Val Acc: 0.8710\n",
            "Epoch [45/300], Train Loss: 0.1266, Train Acc: 0.9546, Val Loss: 0.4681, Val Acc: 0.8668\n",
            "Epoch [46/300], Train Loss: 0.1314, Train Acc: 0.9528, Val Loss: 0.4621, Val Acc: 0.8699\n",
            "Epoch [47/300], Train Loss: 0.1171, Train Acc: 0.9586, Val Loss: 0.4956, Val Acc: 0.8667\n",
            "Epoch [48/300], Train Loss: 0.1163, Train Acc: 0.9584, Val Loss: 0.5850, Val Acc: 0.8553\n",
            "Epoch [49/300], Train Loss: 0.1141, Train Acc: 0.9597, Val Loss: 0.4788, Val Acc: 0.8649\n",
            "Epoch [50/300], Train Loss: 0.1119, Train Acc: 0.9601, Val Loss: 0.4605, Val Acc: 0.8733\n",
            "Epoch [51/300], Train Loss: 0.1077, Train Acc: 0.9614, Val Loss: 0.4808, Val Acc: 0.8658\n",
            "Epoch [52/300], Train Loss: 0.1022, Train Acc: 0.9637, Val Loss: 0.5278, Val Acc: 0.8618\n",
            "Epoch [53/300], Train Loss: 0.1032, Train Acc: 0.9629, Val Loss: 0.5133, Val Acc: 0.8617\n",
            "Epoch [54/300], Train Loss: 0.0958, Train Acc: 0.9661, Val Loss: 0.5500, Val Acc: 0.8572\n",
            "Epoch [55/300], Train Loss: 0.0958, Train Acc: 0.9671, Val Loss: 0.4733, Val Acc: 0.8804\n",
            "Epoch [56/300], Train Loss: 0.0906, Train Acc: 0.9678, Val Loss: 0.4868, Val Acc: 0.8747\n",
            "Epoch [57/300], Train Loss: 0.0901, Train Acc: 0.9690, Val Loss: 0.5473, Val Acc: 0.8603\n",
            "Epoch [58/300], Train Loss: 0.0875, Train Acc: 0.9693, Val Loss: 0.4486, Val Acc: 0.8800\n",
            "Epoch [59/300], Train Loss: 0.0840, Train Acc: 0.9702, Val Loss: 0.4904, Val Acc: 0.8749\n",
            "Epoch [60/300], Train Loss: 0.0831, Train Acc: 0.9706, Val Loss: 0.4799, Val Acc: 0.8798\n",
            "Epoch [61/300], Train Loss: 0.0807, Train Acc: 0.9714, Val Loss: 0.4838, Val Acc: 0.8768\n",
            "Epoch [62/300], Train Loss: 0.0797, Train Acc: 0.9728, Val Loss: 0.5593, Val Acc: 0.8618\n",
            "Epoch [63/300], Train Loss: 0.0782, Train Acc: 0.9722, Val Loss: 0.5933, Val Acc: 0.8538\n",
            "Epoch [64/300], Train Loss: 0.0751, Train Acc: 0.9738, Val Loss: 0.4893, Val Acc: 0.8763\n",
            "Epoch [65/300], Train Loss: 0.0788, Train Acc: 0.9713, Val Loss: 0.4338, Val Acc: 0.8822\n",
            "Epoch [66/300], Train Loss: 0.0720, Train Acc: 0.9747, Val Loss: 0.5298, Val Acc: 0.8714\n",
            "Epoch [67/300], Train Loss: 0.0726, Train Acc: 0.9739, Val Loss: 0.5179, Val Acc: 0.8714\n",
            "Epoch [68/300], Train Loss: 0.0666, Train Acc: 0.9768, Val Loss: 0.5086, Val Acc: 0.8750\n",
            "Epoch [69/300], Train Loss: 0.0697, Train Acc: 0.9755, Val Loss: 0.5422, Val Acc: 0.8726\n",
            "Epoch [70/300], Train Loss: 0.0669, Train Acc: 0.9758, Val Loss: 0.5350, Val Acc: 0.8711\n",
            "Epoch [71/300], Train Loss: 0.0650, Train Acc: 0.9767, Val Loss: 0.5576, Val Acc: 0.8635\n",
            "Epoch [72/300], Train Loss: 0.0624, Train Acc: 0.9778, Val Loss: 0.4824, Val Acc: 0.8841\n",
            "Epoch [73/300], Train Loss: 0.0600, Train Acc: 0.9791, Val Loss: 0.5287, Val Acc: 0.8783\n",
            "Epoch [74/300], Train Loss: 0.0637, Train Acc: 0.9780, Val Loss: 0.5025, Val Acc: 0.8737\n",
            "Epoch [75/300], Train Loss: 0.0569, Train Acc: 0.9794, Val Loss: 0.4924, Val Acc: 0.8761\n",
            "Epoch [76/300], Train Loss: 0.0585, Train Acc: 0.9796, Val Loss: 0.5212, Val Acc: 0.8809\n",
            "Epoch [77/300], Train Loss: 0.0586, Train Acc: 0.9786, Val Loss: 0.5140, Val Acc: 0.8807\n",
            "Epoch [78/300], Train Loss: 0.0588, Train Acc: 0.9798, Val Loss: 0.4843, Val Acc: 0.8824\n",
            "Epoch [79/300], Train Loss: 0.0527, Train Acc: 0.9813, Val Loss: 0.4960, Val Acc: 0.8807\n",
            "Epoch [80/300], Train Loss: 0.0534, Train Acc: 0.9805, Val Loss: 0.5068, Val Acc: 0.8817\n",
            "Epoch [81/300], Train Loss: 0.0517, Train Acc: 0.9819, Val Loss: 0.5021, Val Acc: 0.8830\n",
            "Epoch [82/300], Train Loss: 0.0513, Train Acc: 0.9814, Val Loss: 0.5557, Val Acc: 0.8764\n",
            "Epoch [83/300], Train Loss: 0.0540, Train Acc: 0.9812, Val Loss: 0.4946, Val Acc: 0.8862\n",
            "Epoch [84/300], Train Loss: 0.0521, Train Acc: 0.9825, Val Loss: 0.4819, Val Acc: 0.8843\n",
            "Epoch [85/300], Train Loss: 0.0471, Train Acc: 0.9835, Val Loss: 0.5083, Val Acc: 0.8832\n",
            "Epoch [86/300], Train Loss: 0.0480, Train Acc: 0.9833, Val Loss: 0.4832, Val Acc: 0.8878\n",
            "Epoch [87/300], Train Loss: 0.0494, Train Acc: 0.9829, Val Loss: 0.5632, Val Acc: 0.8707\n",
            "Epoch [88/300], Train Loss: 0.0487, Train Acc: 0.9829, Val Loss: 0.5118, Val Acc: 0.8846\n",
            "Epoch [89/300], Train Loss: 0.0454, Train Acc: 0.9840, Val Loss: 0.5438, Val Acc: 0.8754\n",
            "Epoch [90/300], Train Loss: 0.0462, Train Acc: 0.9835, Val Loss: 0.5359, Val Acc: 0.8818\n",
            "Epoch [91/300], Train Loss: 0.0477, Train Acc: 0.9832, Val Loss: 0.5194, Val Acc: 0.8835\n",
            "Epoch [92/300], Train Loss: 0.0460, Train Acc: 0.9838, Val Loss: 0.4945, Val Acc: 0.8812\n",
            "Epoch [93/300], Train Loss: 0.0449, Train Acc: 0.9837, Val Loss: 0.5253, Val Acc: 0.8816\n",
            "Epoch [94/300], Train Loss: 0.0449, Train Acc: 0.9850, Val Loss: 0.5512, Val Acc: 0.8783\n",
            "Epoch [95/300], Train Loss: 0.0407, Train Acc: 0.9859, Val Loss: 0.5070, Val Acc: 0.8884\n",
            "Epoch [96/300], Train Loss: 0.0415, Train Acc: 0.9858, Val Loss: 0.5542, Val Acc: 0.8747\n",
            "Epoch [97/300], Train Loss: 0.0414, Train Acc: 0.9860, Val Loss: 0.5231, Val Acc: 0.8822\n",
            "Epoch [98/300], Train Loss: 0.0431, Train Acc: 0.9849, Val Loss: 0.5095, Val Acc: 0.8857\n",
            "Epoch [99/300], Train Loss: 0.0413, Train Acc: 0.9856, Val Loss: 0.4802, Val Acc: 0.8901\n",
            "Epoch [100/300], Train Loss: 0.0363, Train Acc: 0.9876, Val Loss: 0.5120, Val Acc: 0.8845\n",
            "Epoch [101/300], Train Loss: 0.0385, Train Acc: 0.9866, Val Loss: 0.5235, Val Acc: 0.8855\n",
            "Epoch [102/300], Train Loss: 0.0385, Train Acc: 0.9869, Val Loss: 0.5181, Val Acc: 0.8858\n",
            "Epoch [103/300], Train Loss: 0.0368, Train Acc: 0.9872, Val Loss: 0.5033, Val Acc: 0.8874\n",
            "Epoch [104/300], Train Loss: 0.0385, Train Acc: 0.9867, Val Loss: 0.5277, Val Acc: 0.8811\n",
            "Epoch [105/300], Train Loss: 0.0370, Train Acc: 0.9874, Val Loss: 0.5121, Val Acc: 0.8861\n",
            "Epoch [106/300], Train Loss: 0.0349, Train Acc: 0.9879, Val Loss: 0.4978, Val Acc: 0.8898\n",
            "Epoch [107/300], Train Loss: 0.0356, Train Acc: 0.9877, Val Loss: 0.5196, Val Acc: 0.8885\n",
            "Epoch [108/300], Train Loss: 0.0335, Train Acc: 0.9887, Val Loss: 0.5339, Val Acc: 0.8820\n",
            "Epoch [109/300], Train Loss: 0.0350, Train Acc: 0.9878, Val Loss: 0.5052, Val Acc: 0.8866\n",
            "Epoch [110/300], Train Loss: 0.0318, Train Acc: 0.9896, Val Loss: 0.5177, Val Acc: 0.8881\n",
            "Epoch [111/300], Train Loss: 0.0331, Train Acc: 0.9887, Val Loss: 0.5785, Val Acc: 0.8795\n",
            "Epoch [112/300], Train Loss: 0.0337, Train Acc: 0.9885, Val Loss: 0.4895, Val Acc: 0.8927\n",
            "Epoch [113/300], Train Loss: 0.0348, Train Acc: 0.9877, Val Loss: 0.5025, Val Acc: 0.8857\n",
            "Epoch [114/300], Train Loss: 0.0308, Train Acc: 0.9895, Val Loss: 0.5015, Val Acc: 0.8929\n",
            "Epoch [115/300], Train Loss: 0.0302, Train Acc: 0.9896, Val Loss: 0.4986, Val Acc: 0.8931\n",
            "Epoch [116/300], Train Loss: 0.0309, Train Acc: 0.9899, Val Loss: 0.4949, Val Acc: 0.8904\n",
            "Epoch [117/300], Train Loss: 0.0296, Train Acc: 0.9900, Val Loss: 0.5097, Val Acc: 0.8925\n",
            "Epoch [118/300], Train Loss: 0.0327, Train Acc: 0.9888, Val Loss: 0.4859, Val Acc: 0.8937\n",
            "Epoch [119/300], Train Loss: 0.0320, Train Acc: 0.9886, Val Loss: 0.4896, Val Acc: 0.8940\n",
            "Epoch [120/300], Train Loss: 0.0308, Train Acc: 0.9890, Val Loss: 0.5447, Val Acc: 0.8821\n",
            "Epoch [121/300], Train Loss: 0.0323, Train Acc: 0.9889, Val Loss: 0.4755, Val Acc: 0.8936\n",
            "Epoch [122/300], Train Loss: 0.0332, Train Acc: 0.9880, Val Loss: 0.5359, Val Acc: 0.8889\n",
            "Epoch [123/300], Train Loss: 0.0305, Train Acc: 0.9894, Val Loss: 0.5593, Val Acc: 0.8819\n",
            "Epoch [124/300], Train Loss: 0.0264, Train Acc: 0.9907, Val Loss: 0.5118, Val Acc: 0.8897\n",
            "Epoch [125/300], Train Loss: 0.0295, Train Acc: 0.9899, Val Loss: 0.5351, Val Acc: 0.8903\n",
            "Epoch [126/300], Train Loss: 0.0268, Train Acc: 0.9907, Val Loss: 0.5303, Val Acc: 0.8870\n",
            "Epoch [127/300], Train Loss: 0.0332, Train Acc: 0.9884, Val Loss: 0.4934, Val Acc: 0.8923\n",
            "Epoch [128/300], Train Loss: 0.0291, Train Acc: 0.9900, Val Loss: 0.4711, Val Acc: 0.9001\n",
            "Epoch [129/300], Train Loss: 0.0283, Train Acc: 0.9902, Val Loss: 0.4886, Val Acc: 0.8945\n",
            "Epoch [130/300], Train Loss: 0.0291, Train Acc: 0.9891, Val Loss: 0.5434, Val Acc: 0.8855\n",
            "Epoch [131/300], Train Loss: 0.0273, Train Acc: 0.9907, Val Loss: 0.5119, Val Acc: 0.8905\n",
            "Epoch [132/300], Train Loss: 0.0268, Train Acc: 0.9903, Val Loss: 0.5068, Val Acc: 0.8889\n",
            "Epoch [133/300], Train Loss: 0.0265, Train Acc: 0.9905, Val Loss: 0.5097, Val Acc: 0.8944\n",
            "Epoch [134/300], Train Loss: 0.0276, Train Acc: 0.9906, Val Loss: 0.5633, Val Acc: 0.8851\n",
            "Epoch [135/300], Train Loss: 0.0280, Train Acc: 0.9906, Val Loss: 0.5068, Val Acc: 0.8910\n",
            "Epoch [136/300], Train Loss: 0.0248, Train Acc: 0.9913, Val Loss: 0.4984, Val Acc: 0.8965\n",
            "Epoch [137/300], Train Loss: 0.0273, Train Acc: 0.9905, Val Loss: 0.5421, Val Acc: 0.8878\n",
            "Epoch [138/300], Train Loss: 0.0271, Train Acc: 0.9905, Val Loss: 0.5310, Val Acc: 0.8900\n",
            "Epoch [139/300], Train Loss: 0.0254, Train Acc: 0.9912, Val Loss: 0.5414, Val Acc: 0.8862\n",
            "Epoch [140/300], Train Loss: 0.0274, Train Acc: 0.9903, Val Loss: 0.5314, Val Acc: 0.8900\n",
            "Epoch [141/300], Train Loss: 0.0283, Train Acc: 0.9903, Val Loss: 0.4852, Val Acc: 0.8922\n",
            "Epoch [142/300], Train Loss: 0.0254, Train Acc: 0.9914, Val Loss: 0.5246, Val Acc: 0.8910\n",
            "Epoch [143/300], Train Loss: 0.0248, Train Acc: 0.9911, Val Loss: 0.4974, Val Acc: 0.8952\n",
            "Epoch [144/300], Train Loss: 0.0268, Train Acc: 0.9911, Val Loss: 0.5564, Val Acc: 0.8919\n",
            "Epoch [145/300], Train Loss: 0.0261, Train Acc: 0.9904, Val Loss: 0.5081, Val Acc: 0.8905\n",
            "Epoch [146/300], Train Loss: 0.0264, Train Acc: 0.9903, Val Loss: 0.4970, Val Acc: 0.8942\n",
            "Epoch [147/300], Train Loss: 0.0234, Train Acc: 0.9921, Val Loss: 0.5546, Val Acc: 0.8875\n",
            "Epoch [148/300], Train Loss: 0.0250, Train Acc: 0.9917, Val Loss: 0.4958, Val Acc: 0.8972\n",
            "Epoch [149/300], Train Loss: 0.0240, Train Acc: 0.9914, Val Loss: 0.5307, Val Acc: 0.8896\n",
            "Epoch [150/300], Train Loss: 0.0260, Train Acc: 0.9906, Val Loss: 0.5101, Val Acc: 0.8918\n",
            "Epoch [151/300], Train Loss: 0.0220, Train Acc: 0.9924, Val Loss: 0.5282, Val Acc: 0.8914\n",
            "Epoch [152/300], Train Loss: 0.0198, Train Acc: 0.9932, Val Loss: 0.5249, Val Acc: 0.8951\n",
            "Epoch [153/300], Train Loss: 0.0225, Train Acc: 0.9925, Val Loss: 0.5133, Val Acc: 0.8948\n",
            "Epoch [154/300], Train Loss: 0.0225, Train Acc: 0.9925, Val Loss: 0.5141, Val Acc: 0.8930\n",
            "Epoch [155/300], Train Loss: 0.0246, Train Acc: 0.9910, Val Loss: 0.4861, Val Acc: 0.8959\n",
            "Epoch [156/300], Train Loss: 0.0230, Train Acc: 0.9921, Val Loss: 0.5141, Val Acc: 0.8946\n",
            "Epoch [157/300], Train Loss: 0.0216, Train Acc: 0.9928, Val Loss: 0.5139, Val Acc: 0.8948\n",
            "Epoch [158/300], Train Loss: 0.0241, Train Acc: 0.9919, Val Loss: 0.5191, Val Acc: 0.8937\n",
            "Epoch [159/300], Train Loss: 0.0208, Train Acc: 0.9931, Val Loss: 0.4935, Val Acc: 0.8949\n",
            "Epoch [160/300], Train Loss: 0.0217, Train Acc: 0.9926, Val Loss: 0.4765, Val Acc: 0.8997\n",
            "Epoch [161/300], Train Loss: 0.0224, Train Acc: 0.9918, Val Loss: 0.5186, Val Acc: 0.8976\n",
            "Epoch [162/300], Train Loss: 0.0232, Train Acc: 0.9916, Val Loss: 0.5546, Val Acc: 0.8859\n",
            "Epoch [163/300], Train Loss: 0.0228, Train Acc: 0.9917, Val Loss: 0.5226, Val Acc: 0.8904\n",
            "Epoch [164/300], Train Loss: 0.0207, Train Acc: 0.9929, Val Loss: 0.5298, Val Acc: 0.8956\n",
            "Epoch [165/300], Train Loss: 0.0227, Train Acc: 0.9920, Val Loss: 0.5129, Val Acc: 0.8969\n",
            "Epoch [166/300], Train Loss: 0.0229, Train Acc: 0.9919, Val Loss: 0.5253, Val Acc: 0.8906\n",
            "Epoch [167/300], Train Loss: 0.0200, Train Acc: 0.9932, Val Loss: 0.4905, Val Acc: 0.8955\n",
            "Epoch [168/300], Train Loss: 0.0197, Train Acc: 0.9933, Val Loss: 0.5076, Val Acc: 0.8931\n",
            "Epoch [169/300], Train Loss: 0.0222, Train Acc: 0.9925, Val Loss: 0.5206, Val Acc: 0.8884\n",
            "Epoch [170/300], Train Loss: 0.0218, Train Acc: 0.9926, Val Loss: 0.5135, Val Acc: 0.8924\n",
            "Epoch [171/300], Train Loss: 0.0196, Train Acc: 0.9934, Val Loss: 0.5104, Val Acc: 0.8959\n",
            "Epoch [172/300], Train Loss: 0.0179, Train Acc: 0.9940, Val Loss: 0.5650, Val Acc: 0.8885\n",
            "Epoch [173/300], Train Loss: 0.0210, Train Acc: 0.9931, Val Loss: 0.5334, Val Acc: 0.8926\n",
            "Epoch [174/300], Train Loss: 0.0234, Train Acc: 0.9923, Val Loss: 0.5139, Val Acc: 0.8926\n",
            "Epoch [175/300], Train Loss: 0.0210, Train Acc: 0.9926, Val Loss: 0.5049, Val Acc: 0.8942\n",
            "Epoch [176/300], Train Loss: 0.0209, Train Acc: 0.9930, Val Loss: 0.5289, Val Acc: 0.8926\n",
            "Epoch [177/300], Train Loss: 0.0217, Train Acc: 0.9928, Val Loss: 0.5244, Val Acc: 0.8894\n",
            "Epoch [178/300], Train Loss: 0.0190, Train Acc: 0.9934, Val Loss: 0.5406, Val Acc: 0.8862\n",
            "Epoch [179/300], Train Loss: 0.0181, Train Acc: 0.9938, Val Loss: 0.5285, Val Acc: 0.8924\n",
            "Epoch [180/300], Train Loss: 0.0205, Train Acc: 0.9928, Val Loss: 0.5027, Val Acc: 0.8982\n",
            "Epoch [181/300], Train Loss: 0.0208, Train Acc: 0.9929, Val Loss: 0.5430, Val Acc: 0.8864\n",
            "Epoch [182/300], Train Loss: 0.0203, Train Acc: 0.9931, Val Loss: 0.5498, Val Acc: 0.8862\n",
            "Epoch [183/300], Train Loss: 0.0192, Train Acc: 0.9932, Val Loss: 0.5074, Val Acc: 0.8939\n",
            "Epoch [184/300], Train Loss: 0.0211, Train Acc: 0.9925, Val Loss: 0.5299, Val Acc: 0.8928\n",
            "Epoch [185/300], Train Loss: 0.0229, Train Acc: 0.9926, Val Loss: 0.5211, Val Acc: 0.8915\n",
            "Epoch [186/300], Train Loss: 0.0208, Train Acc: 0.9929, Val Loss: 0.5092, Val Acc: 0.8945\n",
            "Epoch [187/300], Train Loss: 0.0182, Train Acc: 0.9937, Val Loss: 0.4892, Val Acc: 0.8984\n",
            "Epoch [188/300], Train Loss: 0.0183, Train Acc: 0.9940, Val Loss: 0.5005, Val Acc: 0.8980\n",
            "Epoch [189/300], Train Loss: 0.0192, Train Acc: 0.9934, Val Loss: 0.5636, Val Acc: 0.8868\n",
            "Epoch [190/300], Train Loss: 0.0164, Train Acc: 0.9944, Val Loss: 0.4645, Val Acc: 0.9000\n",
            "Epoch [191/300], Train Loss: 0.0196, Train Acc: 0.9933, Val Loss: 0.5225, Val Acc: 0.8888\n",
            "Epoch [192/300], Train Loss: 0.0196, Train Acc: 0.9936, Val Loss: 0.5305, Val Acc: 0.8911\n",
            "Epoch [193/300], Train Loss: 0.0209, Train Acc: 0.9929, Val Loss: 0.5167, Val Acc: 0.8913\n",
            "Epoch [194/300], Train Loss: 0.0199, Train Acc: 0.9931, Val Loss: 0.4954, Val Acc: 0.8989\n",
            "Epoch [195/300], Train Loss: 0.0185, Train Acc: 0.9938, Val Loss: 0.4947, Val Acc: 0.8999\n",
            "Epoch [196/300], Train Loss: 0.0194, Train Acc: 0.9936, Val Loss: 0.5428, Val Acc: 0.8898\n",
            "Epoch [197/300], Train Loss: 0.0184, Train Acc: 0.9939, Val Loss: 0.5186, Val Acc: 0.8962\n",
            "Epoch [198/300], Train Loss: 0.0214, Train Acc: 0.9927, Val Loss: 0.4999, Val Acc: 0.8956\n",
            "Epoch [199/300], Train Loss: 0.0195, Train Acc: 0.9935, Val Loss: 0.4866, Val Acc: 0.9000\n",
            "Epoch [200/300], Train Loss: 0.0158, Train Acc: 0.9948, Val Loss: 0.5208, Val Acc: 0.8931\n",
            "Epoch [201/300], Train Loss: 0.0189, Train Acc: 0.9936, Val Loss: 0.5613, Val Acc: 0.8864\n",
            "Epoch [202/300], Train Loss: 0.0181, Train Acc: 0.9939, Val Loss: 0.5600, Val Acc: 0.8882\n",
            "Epoch [203/300], Train Loss: 0.0191, Train Acc: 0.9935, Val Loss: 0.4782, Val Acc: 0.9011\n",
            "Epoch [204/300], Train Loss: 0.0193, Train Acc: 0.9935, Val Loss: 0.5319, Val Acc: 0.8904\n",
            "Epoch [205/300], Train Loss: 0.0173, Train Acc: 0.9944, Val Loss: 0.4910, Val Acc: 0.8946\n",
            "Epoch [206/300], Train Loss: 0.0168, Train Acc: 0.9942, Val Loss: 0.5634, Val Acc: 0.8871\n",
            "Epoch [207/300], Train Loss: 0.0177, Train Acc: 0.9939, Val Loss: 0.5088, Val Acc: 0.8935\n",
            "Epoch [208/300], Train Loss: 0.0172, Train Acc: 0.9938, Val Loss: 0.4864, Val Acc: 0.8993\n",
            "Epoch [209/300], Train Loss: 0.0171, Train Acc: 0.9942, Val Loss: 0.5413, Val Acc: 0.8911\n",
            "Epoch [210/300], Train Loss: 0.0191, Train Acc: 0.9934, Val Loss: 0.5057, Val Acc: 0.8918\n",
            "Epoch [211/300], Train Loss: 0.0211, Train Acc: 0.9928, Val Loss: 0.5142, Val Acc: 0.8959\n",
            "Epoch [212/300], Train Loss: 0.0183, Train Acc: 0.9935, Val Loss: 0.5301, Val Acc: 0.8926\n",
            "Epoch [213/300], Train Loss: 0.0181, Train Acc: 0.9939, Val Loss: 0.5381, Val Acc: 0.8893\n",
            "Epoch [214/300], Train Loss: 0.0171, Train Acc: 0.9942, Val Loss: 0.5143, Val Acc: 0.8978\n",
            "Epoch [215/300], Train Loss: 0.0164, Train Acc: 0.9944, Val Loss: 0.5148, Val Acc: 0.8971\n",
            "Epoch [216/300], Train Loss: 0.0182, Train Acc: 0.9940, Val Loss: 0.5218, Val Acc: 0.8908\n",
            "Epoch [217/300], Train Loss: 0.0171, Train Acc: 0.9944, Val Loss: 0.5039, Val Acc: 0.8936\n",
            "Epoch [218/300], Train Loss: 0.0161, Train Acc: 0.9946, Val Loss: 0.5505, Val Acc: 0.8877\n",
            "Epoch [219/300], Train Loss: 0.0185, Train Acc: 0.9937, Val Loss: 0.5389, Val Acc: 0.8937\n",
            "Epoch [220/300], Train Loss: 0.0140, Train Acc: 0.9960, Val Loss: 0.5195, Val Acc: 0.8978\n",
            "Epoch [221/300], Train Loss: 0.0165, Train Acc: 0.9945, Val Loss: 0.5209, Val Acc: 0.8947\n",
            "Epoch [222/300], Train Loss: 0.0176, Train Acc: 0.9941, Val Loss: 0.5021, Val Acc: 0.8997\n",
            "Epoch [223/300], Train Loss: 0.0183, Train Acc: 0.9937, Val Loss: 0.5566, Val Acc: 0.8886\n",
            "Epoch [224/300], Train Loss: 0.0145, Train Acc: 0.9952, Val Loss: 0.5298, Val Acc: 0.8900\n",
            "Epoch [225/300], Train Loss: 0.0163, Train Acc: 0.9947, Val Loss: 0.5478, Val Acc: 0.8893\n",
            "Epoch [226/300], Train Loss: 0.0191, Train Acc: 0.9933, Val Loss: 0.5194, Val Acc: 0.8947\n",
            "Epoch [227/300], Train Loss: 0.0192, Train Acc: 0.9932, Val Loss: 0.4864, Val Acc: 0.9018\n",
            "Epoch [228/300], Train Loss: 0.0168, Train Acc: 0.9945, Val Loss: 0.5139, Val Acc: 0.8958\n",
            "Epoch [229/300], Train Loss: 0.0191, Train Acc: 0.9940, Val Loss: 0.4882, Val Acc: 0.8968\n",
            "Epoch [230/300], Train Loss: 0.0144, Train Acc: 0.9955, Val Loss: 0.4892, Val Acc: 0.8972\n",
            "Epoch [231/300], Train Loss: 0.0153, Train Acc: 0.9945, Val Loss: 0.5066, Val Acc: 0.8968\n",
            "Epoch [232/300], Train Loss: 0.0157, Train Acc: 0.9948, Val Loss: 0.5260, Val Acc: 0.8972\n",
            "Epoch [233/300], Train Loss: 0.0141, Train Acc: 0.9952, Val Loss: 0.5133, Val Acc: 0.8951\n",
            "Epoch [234/300], Train Loss: 0.0172, Train Acc: 0.9939, Val Loss: 0.5122, Val Acc: 0.8936\n",
            "Epoch [235/300], Train Loss: 0.0161, Train Acc: 0.9950, Val Loss: 0.5687, Val Acc: 0.8913\n",
            "Epoch [236/300], Train Loss: 0.0168, Train Acc: 0.9948, Val Loss: 0.4883, Val Acc: 0.8957\n",
            "Epoch [237/300], Train Loss: 0.0139, Train Acc: 0.9959, Val Loss: 0.5390, Val Acc: 0.8938\n",
            "Epoch [238/300], Train Loss: 0.0174, Train Acc: 0.9938, Val Loss: 0.5184, Val Acc: 0.8990\n",
            "Epoch [239/300], Train Loss: 0.0204, Train Acc: 0.9932, Val Loss: 0.5160, Val Acc: 0.8931\n",
            "Epoch [240/300], Train Loss: 0.0184, Train Acc: 0.9938, Val Loss: 0.5533, Val Acc: 0.8908\n",
            "Epoch [241/300], Train Loss: 0.0166, Train Acc: 0.9944, Val Loss: 0.5614, Val Acc: 0.8836\n",
            "Epoch [242/300], Train Loss: 0.0181, Train Acc: 0.9940, Val Loss: 0.5636, Val Acc: 0.8843\n",
            "Epoch [243/300], Train Loss: 0.0171, Train Acc: 0.9944, Val Loss: 0.4985, Val Acc: 0.9000\n",
            "Epoch [244/300], Train Loss: 0.0155, Train Acc: 0.9948, Val Loss: 0.4999, Val Acc: 0.8948\n",
            "Epoch [245/300], Train Loss: 0.0148, Train Acc: 0.9951, Val Loss: 0.5387, Val Acc: 0.8915\n",
            "Epoch [246/300], Train Loss: 0.0155, Train Acc: 0.9946, Val Loss: 0.5003, Val Acc: 0.8962\n",
            "Epoch [247/300], Train Loss: 0.0151, Train Acc: 0.9949, Val Loss: 0.5306, Val Acc: 0.8953\n",
            "Epoch [248/300], Train Loss: 0.0124, Train Acc: 0.9959, Val Loss: 0.5097, Val Acc: 0.9000\n",
            "Epoch [249/300], Train Loss: 0.0168, Train Acc: 0.9948, Val Loss: 0.5354, Val Acc: 0.8962\n",
            "Epoch [250/300], Train Loss: 0.0144, Train Acc: 0.9955, Val Loss: 0.5820, Val Acc: 0.8807\n",
            "Epoch [251/300], Train Loss: 0.0136, Train Acc: 0.9953, Val Loss: 0.5421, Val Acc: 0.8912\n",
            "Epoch [252/300], Train Loss: 0.0170, Train Acc: 0.9943, Val Loss: 0.5857, Val Acc: 0.8852\n",
            "Epoch [253/300], Train Loss: 0.0179, Train Acc: 0.9942, Val Loss: 0.5071, Val Acc: 0.8935\n",
            "Epoch [254/300], Train Loss: 0.0149, Train Acc: 0.9950, Val Loss: 0.4760, Val Acc: 0.9014\n",
            "Epoch [255/300], Train Loss: 0.0156, Train Acc: 0.9951, Val Loss: 0.5091, Val Acc: 0.8936\n",
            "Epoch [256/300], Train Loss: 0.0179, Train Acc: 0.9937, Val Loss: 0.5017, Val Acc: 0.8960\n",
            "Epoch [257/300], Train Loss: 0.0164, Train Acc: 0.9945, Val Loss: 0.4753, Val Acc: 0.9006\n",
            "Epoch [258/300], Train Loss: 0.0164, Train Acc: 0.9947, Val Loss: 0.4885, Val Acc: 0.9003\n",
            "Epoch [259/300], Train Loss: 0.0169, Train Acc: 0.9946, Val Loss: 0.4874, Val Acc: 0.9000\n",
            "Epoch [260/300], Train Loss: 0.0128, Train Acc: 0.9959, Val Loss: 0.5163, Val Acc: 0.8972\n",
            "Epoch [261/300], Train Loss: 0.0157, Train Acc: 0.9947, Val Loss: 0.5161, Val Acc: 0.8934\n",
            "Epoch [262/300], Train Loss: 0.0167, Train Acc: 0.9943, Val Loss: 0.4883, Val Acc: 0.8978\n",
            "Epoch [263/300], Train Loss: 0.0146, Train Acc: 0.9954, Val Loss: 0.5731, Val Acc: 0.8917\n",
            "Epoch [264/300], Train Loss: 0.0146, Train Acc: 0.9948, Val Loss: 0.5238, Val Acc: 0.8953\n",
            "Epoch [265/300], Train Loss: 0.0187, Train Acc: 0.9936, Val Loss: 0.5026, Val Acc: 0.8990\n",
            "Epoch [266/300], Train Loss: 0.0170, Train Acc: 0.9940, Val Loss: 0.5336, Val Acc: 0.8903\n",
            "Epoch [267/300], Train Loss: 0.0178, Train Acc: 0.9940, Val Loss: 0.5176, Val Acc: 0.8982\n",
            "Epoch [268/300], Train Loss: 0.0133, Train Acc: 0.9953, Val Loss: 0.4980, Val Acc: 0.8959\n",
            "Epoch [269/300], Train Loss: 0.0169, Train Acc: 0.9938, Val Loss: 0.5391, Val Acc: 0.8911\n",
            "Epoch [270/300], Train Loss: 0.0146, Train Acc: 0.9948, Val Loss: 0.5184, Val Acc: 0.8947\n",
            "Epoch [271/300], Train Loss: 0.0175, Train Acc: 0.9938, Val Loss: 0.5153, Val Acc: 0.8947\n",
            "Epoch [272/300], Train Loss: 0.0157, Train Acc: 0.9948, Val Loss: 0.5339, Val Acc: 0.8934\n",
            "Epoch [273/300], Train Loss: 0.0172, Train Acc: 0.9943, Val Loss: 0.5487, Val Acc: 0.8867\n",
            "Epoch [274/300], Train Loss: 0.0164, Train Acc: 0.9949, Val Loss: 0.5070, Val Acc: 0.8913\n",
            "Epoch [275/300], Train Loss: 0.0135, Train Acc: 0.9955, Val Loss: 0.5392, Val Acc: 0.8914\n",
            "Epoch [276/300], Train Loss: 0.0163, Train Acc: 0.9942, Val Loss: 0.4797, Val Acc: 0.9001\n",
            "Epoch [277/300], Train Loss: 0.0175, Train Acc: 0.9939, Val Loss: 0.5114, Val Acc: 0.8949\n",
            "Epoch [278/300], Train Loss: 0.0181, Train Acc: 0.9936, Val Loss: 0.5033, Val Acc: 0.8958\n",
            "Epoch [279/300], Train Loss: 0.0157, Train Acc: 0.9949, Val Loss: 0.5279, Val Acc: 0.8936\n",
            "Epoch [280/300], Train Loss: 0.0149, Train Acc: 0.9950, Val Loss: 0.5228, Val Acc: 0.8891\n",
            "Epoch [281/300], Train Loss: 0.0145, Train Acc: 0.9951, Val Loss: 0.5172, Val Acc: 0.8948\n",
            "Epoch [282/300], Train Loss: 0.0143, Train Acc: 0.9955, Val Loss: 0.5147, Val Acc: 0.8997\n",
            "Epoch [283/300], Train Loss: 0.0178, Train Acc: 0.9940, Val Loss: 0.5192, Val Acc: 0.8933\n",
            "Epoch [284/300], Train Loss: 0.0174, Train Acc: 0.9941, Val Loss: 0.5220, Val Acc: 0.8920\n",
            "Epoch [285/300], Train Loss: 0.0177, Train Acc: 0.9941, Val Loss: 0.5122, Val Acc: 0.8942\n",
            "Epoch [286/300], Train Loss: 0.0168, Train Acc: 0.9944, Val Loss: 0.5183, Val Acc: 0.8965\n",
            "Epoch [287/300], Train Loss: 0.0161, Train Acc: 0.9944, Val Loss: 0.4896, Val Acc: 0.8980\n",
            "Epoch [288/300], Train Loss: 0.0142, Train Acc: 0.9953, Val Loss: 0.5095, Val Acc: 0.8954\n",
            "Epoch [289/300], Train Loss: 0.0160, Train Acc: 0.9944, Val Loss: 0.5266, Val Acc: 0.8967\n",
            "Epoch [290/300], Train Loss: 0.0148, Train Acc: 0.9952, Val Loss: 0.5298, Val Acc: 0.8933\n",
            "Epoch [291/300], Train Loss: 0.0153, Train Acc: 0.9947, Val Loss: 0.5184, Val Acc: 0.8963\n",
            "Epoch [292/300], Train Loss: 0.0133, Train Acc: 0.9957, Val Loss: 0.5291, Val Acc: 0.8921\n",
            "Epoch [293/300], Train Loss: 0.0157, Train Acc: 0.9943, Val Loss: 0.5302, Val Acc: 0.8969\n",
            "Epoch [294/300], Train Loss: 0.0169, Train Acc: 0.9944, Val Loss: 0.5002, Val Acc: 0.8966\n",
            "Epoch [295/300], Train Loss: 0.0154, Train Acc: 0.9950, Val Loss: 0.4978, Val Acc: 0.8987\n",
            "Epoch [296/300], Train Loss: 0.0161, Train Acc: 0.9947, Val Loss: 0.5178, Val Acc: 0.8934\n",
            "Epoch [297/300], Train Loss: 0.0136, Train Acc: 0.9957, Val Loss: 0.5211, Val Acc: 0.8925\n",
            "Epoch [298/300], Train Loss: 0.0150, Train Acc: 0.9947, Val Loss: 0.4895, Val Acc: 0.9024\n",
            "Epoch [299/300], Train Loss: 0.0167, Train Acc: 0.9945, Val Loss: 0.4786, Val Acc: 0.8952\n",
            "Epoch [300/300], Train Loss: 0.0154, Train Acc: 0.9950, Val Loss: 0.5121, Val Acc: 0.8975\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fT5x62nPpxV6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}